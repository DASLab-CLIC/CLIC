# TODO: 资源描述 如何 决定
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: $name$ # 填入stage的名称
spec:
  type: Scala
  mode: cluster
  image: $image$ # 填入image的地址
  imagePullPolicy: $imagePolicy$ # 平台每次需要更新成本较高，测试阶段使用Always
  mainClass: $mainClass$ # 填入主函数
  mainApplicationFile: "local:///$mainJar$" # 填入jar包地址，先将spark代码打包进来
  sparkVersion: $sparkVersion$ # 填入spark的版本
  arguments: ["$argument$"] # 传入的参数
  sparkConf:
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "file:///data/logs" # spark的日志存放路径
#  sparkConfigMap: spark-conf-map # 指定configmap，这里为了方便查看日志
  restartPolicy:
    type: Never
  volumes:
  - name: nfs-volume
    persistentVolumeClaim:
      claimName: pvc-nfs
  driver:
    cores: 1
    memory: "512m"
    labels:
      version: $sparkVersion$ # 填入spark的版本
    serviceAccount: spark # 意味着要创建一个spark的sa
#    javaOptions: "-Dlog4j.configuration=local:///opt/spark/conf/log4j.properties"
    volumeMounts:
    - name: nfs-volume
      mountPath: /data
  executor:
    cores: 6
    instances: 3
    memory: "512m"
#    javaOptions: "-Dlog4j.configuration=local:///opt/spark/conf/log4j.properties"
    labels:
      version: $sparkVersion$ # 填入spark的版本
    volumeMounts:
    - name: nfs-volume
      mountPath: /data